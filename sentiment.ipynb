{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sentiment:\n",
    "#     def __init__(self):\n",
    "#         import pandas as pd\n",
    "#         self.pd=pd\n",
    "#         import numpy as np\n",
    "#         self.np=np\n",
    "#         import seaborn as sns\n",
    "#         self.sns=sns\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         self.plt=plt\n",
    "#         import pipes\n",
    "#         from pipeline import Pipeline\n",
    "#         self.Pipeline=Pipeline\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>**Steps to Follow**</font>\n",
    "1. Loading and Exploring Data\n",
    "2. Text Cleaning\n",
    "3. Data Preparation\n",
    "    1. Label Encoding\n",
    "    2. Split Data\n",
    "    3. Feature Engineering using TF-IDF\n",
    "4. Model Building\n",
    "    1. Naive Bayes\n",
    "    2. Logistic Regression\n",
    "    3. Model Building Summary\n",
    "5. Final Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #for handling data\n",
    "import numpy as np #for numerical computing\n",
    "import seaborn as sns #ploting\n",
    "import matplotlib.pyplot as plt #ploting\n",
    "\n",
    "import re #for text data cleaning/Library for pattern matching\n",
    "\n",
    "# for NLP related tasks\n",
    "import spacy \n",
    "nlp=spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"tweets 2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316      @virginamerica the manage itinerary section of...\n",
       "23       @VirginAmerica will you be making BOS&gt;LAS n...\n",
       "9629     @USAirways on a happy note our 719 crew is won...\n",
       "8599                             @JetBlue I do follow you!\n",
       "14239    @AmericanAir â€¦ Been trying to book a whole new...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"airline_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    62.691257\n",
       "neutral     21.168033\n",
       "positive    16.140710\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution in percentage\n",
    "df[\"airline_sentiment\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_claner(text):\n",
    "     #remove user mentions\n",
    "     text=re.sub(r\"@[A-Za-z0-9]+\",\"\",text)\n",
    "      # remove hashtags\n",
    "     text=re.sub(r'#[A-Za-z0-9]+','',text)\n",
    "     #remove links\n",
    "     text=re.sub(r'http\\S+','',text)\n",
    "       #convering text to lower case\n",
    "     text=text.lower()  \n",
    "     #fetch only words\n",
    "     text=re.sub(r'[^a-z]+',\" \",text)\n",
    "     # removing extra spaces\n",
    "     text=re.sub(r'[\\s]+',' ',text)\n",
    "     # creating doc object\n",
    "     doc=nlp(text)\n",
    "    # remove stopwords and lemmatize the text\n",
    "     tokens=[token.lemma_ for token in doc if(token.is_stop==False)]\n",
    "\n",
    "    #  tokens=[tokens.lemma_ for token in doc if(token.is_stop==False)]\n",
    "      #join tokens by space\n",
    "     return \" \".join(tokens) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>plus ve add commercial experience tacky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "                                cleaned_text  \n",
       "0                                        say  \n",
       "1    plus ve add commercial experience tacky  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform text cleaning\n",
    "df[\"cleaned_text\"]=df[\"text\"].apply(text_claner)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned text and labels to a variable\n",
    "text=df[\"cleaned_text\"].values\n",
    "\n",
    "labels=df[\"airline_sentiment\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['  say', '  plus ve add commercial experience tacky',\n",
       "       '  didn t today mean need trip',\n",
       "       '  s aggressive blast obnoxious entertainment guest face amp little recourse',\n",
       "       '  s big bad thing'], dtype=object)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'neutral' 'negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 0, 0, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "labels=le.fit_transform(labels)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Meaning of each label\n",
    "le.inverse_transform(np.array([0,1,2])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=text\n",
    "y=labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,stratify=y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712,) (11712,)\n",
      "(2928,) (2928,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize TFIDF\n",
    "tfidf=TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Vectorizer on Train set\n",
    "word_vec=tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=1000)\n"
     ]
    }
   ],
   "source": [
    "print(word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'today': 892, 'bit': 95, 'bag': 79, 'issue': 469, 'clear': 155, 'thank': 879, 'check': 143, 'hour': 439, 'gate': 392, 'hold': 430, 'minute': 580, 'people': 651, 'flight': 350, 'go': 398, 'experience': 311, 'terrible': 876, 'awful': 76, 'customer': 215, 'service': 789, 'lose': 528, 'win': 973, 'free': 373, 'kid': 484, 'leave': 501, 'onboard': 623, 'ua': 919, 'ord': 629, 'sfo': 791, 'row': 758, 'help': 423, 'offer': 616, 'sleep': 809, 'cancel': 123, 'flightle': 355, 'fucking': 383, 'hotel': 438, 'like': 511, 'suppose': 858, 'provide': 702, 'love': 532, 'know': 487, 'policy': 677, 'luggage': 538, 'lounge': 531, 'attendant': 62, 'terminal': 875, 'give': 396, 'avail': 70, 'hi': 426, 'booking': 102, 'problem': 695, 'number': 613, 'sure': 859, 'call': 121, 'rebooke': 720, 'find': 342, 'taxi': 872, 'stand': 831, 'las': 495, 'vegas': 943, 'say': 773, 'time': 887, 'rep': 734, 'tell': 874, 'denver': 237, 'nope': 607, 'currently': 213, 'sit': 804, 'friend': 377, 'have': 417, 'flightlation': 354, 'lax': 497, 'cmh': 164, 'feb': 332, 'year': 993, 'think': 881, 'fight': 336, 'bad': 78, 'airplane': 25, 'add': 12, 'air': 21, 've': 942, 'min': 577, 'sort': 820, 'reservation': 742, 'message': 568, 'try': 910, 'wrong': 989, 'guy': 406, 'suck': 849, 'jfk': 477, 'tonight': 894, 'oh': 618, 'morning': 589, 'correct': 197, 'sorry': 819, 'couldn': 199, 'book': 101, 'amp': 34, 'security': 781, 'boarding': 100, 'pass': 641, 'treat': 904, 'airline': 23, 'life': 509, 'military': 576, 'family': 324, 'fly': 361, 'cheer': 146, 'agent': 18, 'original': 631, 'plane': 668, 'get': 393, 'de': 227, 'ice': 448, 'engine': 289, 'short': 795, 'take': 867, 'dme': 267, 'confirmation': 187, 'den': 236, 'schedule': 774, 'land': 492, 'rebook': 719, 'good': 400, 'hand': 409, 'airport': 26, 'wait': 950, 'shit': 794, 'mad': 540, 'come': 170, 'broken': 108, 'seat': 777, 'fix': 346, 'sign': 800, 'want': 954, 'warm': 955, 'crew': 209, 'logan': 524, 'dc': 225, 'area': 50, 'baggage': 80, 'dallas': 219, 'miss': 581, 'connection': 190, 'instead': 464, 'use': 938, 'hate': 416, 'internet': 466, 'lol': 525, 'far': 327, 'll': 520, 'child': 148, 'ahead': 20, 'cause': 132, 'nice': 602, 'route': 757, 'jet': 474, 'blue': 97, 'southwest': 823, 'iad': 446, 'mechanical': 559, 'delay': 231, 'pay': 648, 'choice': 149, 'big': 91, 'man': 546, 'continue': 193, 'submit': 848, 'damage': 220, 'complaint': 179, 'online': 625, 'way': 960, 'ago': 19, 'website': 963, 'show': 798, 'awesome': 75, 'new': 599, 'extremely': 317, 'captain': 124, 'ticket': 885, 'mile': 574, 'absolute': 2, 'joke': 479, 'need': 598, 'lot': 530, 'work': 981, 'partner': 639, 'trip': 905, 'buy': 117, 'conf': 185, 'ok': 619, 'feedback': 334, 'line': 513, 'dca': 226, 'talk': 869, 'tomorrow': 893, 'hasn': 415, 'hear': 420, 'week': 964, 'nonstop': 606, 'steal': 837, 'follow': 364, 'dm': 266, 'learn': 500, 'switch': 864, 'eat': 282, 'difference': 255, 'plan': 667, 'sun': 853, 'crazy': 207, 'haven': 418, 'speak': 825, 'human': 444, 'well': 967, 'receive': 722, 'email': 285, 'confirm': 186, 'request': 738, 'mileage': 575, 'keep': 482, 'info': 460, 'alert': 29, 'quick': 708, 'response': 747, 'finger': 344, 'cross': 210, 'yesterday': 996, 'employee': 287, 'bna': 98, 'mdw': 555, 'strand': 845, 'day': 224, 'ur': 934, 'houston': 440, 'worth': 985, 'don': 272, 'care': 127, 'hrs': 442, 'late': 496, 'flightr': 357, 'home': 431, 'rental': 733, 'car': 125, 'etc': 298, 'yr': 998, 'old': 621, 'happen': 412, 'priority': 693, 'print': 692, 'desk': 245, 'philadelphia': 655, 'phl': 657, 'actually': 11, 'room': 755, 'reschedule': 741, 'weather': 961, 'let': 503, 'stay': 836, 'nyc': 615, 'move': 590, 'twitter': 917, 'send': 785, 'appreciate': 47, 'wow': 987, 'answer': 38, 'phone': 659, 'look': 527, 'companion': 174, 'ridiculous': 752, 'stop': 841, 'see': 782, 'month': 588, 'commercial': 172, 'tv': 914, 'excited': 305, 'support': 857, 'lovely': 533, 'view': 945, 'personal': 654, 'contact': 192, 'money': 587, 'sick': 799, 'live': 519, 'literally': 517, 'run': 764, 'travel': 902, 'aus': 64, 'direct': 257, 'long': 526, 'connect': 189, 'early': 279, 'stick': 840, 'computer': 182, 'system': 865, 'crash': 206, 'report': 736, 'file': 338, 'th': 878, 'case': 130, 'require': 739, 'open': 626, 'professional': 697, 'post': 681, 'board': 99, 'watch': 958, 'passenger': 643, 'hr': 441, 'ground': 402, 'didn': 251, 'break': 106, 'upset': 933, 'happy': 413, 'wife': 969, 'bday': 87, 'compensate': 176, 'atl': 59, 'card': 126, 'company': 175, 'explain': 313, 'mexico': 569, 'purchase': 704, 'member': 563, 'rt': 760, 'second': 780, 'touch': 897, 'claim': 152, 'soon': 818, 'reimburse': 728, 'airways': 28, 'airway': 27, 'high': 427, 'wall': 953, 'street': 846, 'pm': 675, 'relate': 729, 'flightd': 352, 'story': 843, 'assistance': 58, 'absolutely': 3, 'horrible': 436, 'person': 653, 'different': 256, 'friendly': 378, 'refund': 726, 'voucher': 949, 'drive': 277, 'club': 162, 'exactly': 303, 'usa': 935, 'extra': 316, 'worker': 982, 'oscar': 634, 'doesn': 268, 'carry': 129, 'fail': 320, 'information': 462, 'half': 408, 'maybe': 553, 'update': 931, 'idea': 449, 'announce': 36, 'choose': 150, 'point': 676, 'lack': 490, 'pls': 672, 'low': 534, 'jetblue': 475, 'clue': 163, 'cab': 119, 'thx': 884, 'country': 202, 'away': 74, 'possible': 679, 'little': 518, 'night': 603, 'guess': 405, 'solve': 816, 'charge': 140, 'force': 367, 'hang': 411, 'hook': 433, 'queue': 707, 'hope': 434, 'thing': 880, 'reflight': 725, 'deal': 228, 'kind': 486, 'type': 918, 'credit': 208, 'forward': 372, 'change': 139, 'fee': 333, 'birthday': 94, 'huge': 443, 'fan': 325, 'seriously': 787, 'fll': 358, 'dfw': 249, 'cut': 216, 'enter': 291, 'stewardess': 839, 'water': 959, 'fort': 371, 'cool': 195, 'fl': 347, 'wonder': 978, 'traveler': 903, 'process': 696, 'yes': 995, 'arrive': 53, 'weekend': 965, 'plus': 673, 'able': 1, 'cost': 198, 'stuff': 847, 'layover': 498, 'premium': 688, 'hello': 422, 'account': 8, 'price': 691, 'app': 42, 'fault': 330, 'destination': 247, 'sad': 766, 'busy': 116, 'intl': 467, 'ask': 55, 'advisory': 16, 'start': 833, 'finally': 341, 'failure': 321, 'list': 515, 'prefer': 686, 'web': 962, 'helpful': 424, 'favorite': 331, 'record': 723, 'american': 33, 'flt': 360, 'middle': 572, 'rate': 711, 'excellent': 304, 'beautiful': 88, 'shot': 796, 'share': 793, 'welcome': 966, 'round': 756, 'ceo': 135, 'right': 753, 'daily': 217, 'journal': 480, 'date': 222, 'fare': 328, 'tag': 866, 'excuse': 306, 'actual': 10, 'honor': 432, 'yeah': 992, 'hopefully': 435, 'frustrated': 380, 'san': 770, 'food': 365, 'departure': 241, 'disappointed': 260, 'philly': 656, 'gt': 404, 'mia': 570, 'turn': 913, 'link': 514, 'lead': 499, 'job': 478, 'available': 71, 'afternoon': 17, 'control': 194, 'great': 401, 'trouble': 906, 'flighted': 353, 'effort': 283, 'iah': 447, 'close': 159, 'mco': 554, 'aa': 0, 'monday': 586, 'advise': 15, 'bird': 93, 'depart': 239, 'atlanta': 60, 'reason': 718, 'respond': 746, 'coffee': 167, 'option': 628, 'swa': 863, 'appear': 44, 'fuck': 382, 'communication': 173, 'hard': 414, 'disconnect': 262, 'face': 318, 'congrat': 188, 'cust': 214, 'relation': 730, 'dept': 243, 'mom': 584, 'virgin': 946, 'ride': 751, 'notice': 610, 'music': 593, 'note': 609, 'chicago': 147, 'anymore': 39, 'completely': 181, 'waste': 957, 'friday': 376, 'apology': 41, 'forget': 369, 'load': 521, 'vacation': 941, 'delta': 235, 'chance': 138, 'not': 608, 'you': 997, 'expect': 309, 'unacceptable': 923, 'figure': 337, 'pilot': 665, 'fuel': 384, 'supervisor': 856, 'drink': 276, 'tweet': 915, 'true': 907, 'question': 706, 'power': 682, 'cabin': 120, 'include': 455, 'step': 838, 'dal': 218, 'hell': 421, 'lt': 536, 'save': 772, 'orlando': 633, 'chairman': 137, 'st': 829, 'class': 153, 'kick': 483, 'tix': 889, 'rude': 761, 'cover': 205, 'event': 300, 'transfer': 901, 'ruin': 762, 'serve': 788, 'funny': 388, 'perfect': 652, 'bring': 107, 'order': 630, 'isn': 468, 'cc': 133, 'sister': 803, 'damn': 221, 'redeem': 724, 'pick': 662, 'cold': 168, 'upgrade': 932, 'flightation': 351, 'newark': 600, 'tarmac': 871, 'eventually': 301, 'drop': 278, 'refuse': 727, 'space': 824, 'buffalo': 112, 'mobile': 583, 'meeting': 562, 'luck': 537, 'pbi': 649, 'clt': 161, 'door': 273, 'important': 452, 'yep': 994, 'promise': 699, 'earn': 280, 'count': 200, 'understand': 925, 'shouldn': 797, 'especially': 296, 'wasn': 956, 'reply': 735, 'status': 835, 'unable': 922, 'glad': 397, 'return': 749, 'lost': 529, 'lga': 506, 'poor': 678, 'frequent': 375, 'flyer': 362, 'merger': 566, 'complete': 180, 'minor': 579, 'evening': 299, 'despite': 246, 'unite': 929, 'mail': 541, 'usairway': 937, 'detail': 248, 'spend': 827, 'fleet': 349, 'fleek': 348, 'enjoy': 290, 'bwi': 118, 'error': 295, 'maintenance': 543, 'woman': 977, 'amazing': 31, 'reserve': 743, 'select': 783, 'miami': 571, 'dragon': 275, 'exit': 308, 'throw': 883, 'business': 115, 'inconvenience': 456, 'attitude': 63, 'unfortunately': 926, 'sell': 784, 'jblu': 473, 'jb': 472, 'phx': 661, 'overnight': 637, 'bump': 113, 'world': 983, 'checkin': 145, 'tmrw': 890, 'log': 523, 'wifi': 970, 'frustrating': 381, 'manage': 547, 'mean': 557, 'flightled': 356, 'allow': 30, 'blame': 96, 'easy': 281, 'united': 930, 'airlines': 24, 'nd': 596, 'fast': 329, 'safety': 768, 'daughter': 223, 'sw': 862, 'weren': 968, 'dog': 269, 'hire': 428, 'read': 715, 'disappoint': 259, 'sky': 807, 'resolve': 745, 'takeoff': 868, 'begin': 89, 'brother': 109, 'hot': 437, 'page': 638, 'ny': 614, 'landing': 493, 'super': 855, 'training': 900, 'quickly': 709, 'btw': 111, 'end': 288, 'non': 605, 'deserve': 244, 'head': 419, 'bc': 86, 'aren': 51, 'course': 204, 'lie': 508, 'directly': 258, 'act': 9, 'haha': 407, 'apologize': 40, 'piece': 664, 'wonderful': 979, 'catch': 131, 'access': 6, 'reroute': 740, 'die': 252, 'nightmare': 604, 'counter': 201, 'ewr': 302, 'accommodate': 7, 'situation': 806, 'corporate': 196, 'manager': 548, 'deliver': 233, 'wouldn': 986, 'main': 542, 'group': 403, 'merge': 565, 'department': 240, 'believe': 90, 'fun': 385, 'dividend': 265, 'bother': 105, 'phoenix': 658, 'bos': 103, 'team': 873, 'snack': 812, 'train': 899, 'staff': 830, 'award': 73, 'inform': 461, 'south': 822, 'name': 594, 'expire': 312, 'reach': 714, 'zero': 999, 'florida': 359, 'operate': 627, 'usair': 936, 'truly': 908, 'news': 601, 'dollar': 270, 'mention': 564, 'fact': 319, 'future': 389, 'make': 545, 'msg': 591, 'address': 14, 'fine': 343, 'past': 644, 'itinerary': 471, 'la': 489, 'definitely': 230, 'mistake': 582, 'certificate': 136, 'snow': 813, 'sense': 786, 'city': 151, 'sale': 769, 'rd': 712, 'worry': 984, 'premier': 687, 'probably': 694, 'to': 891, 'acceptable': 5, 'notify': 612, 'apparently': 43, 'trust': 909, 'small': 811, 'word': 980, 'originally': 632, 'getting': 394, 'text': 877, 'wtf': 990, 'cheap': 142, 'condition': 184, 'seating': 778, 'screw': 775, 'pathetic': 645, 'large': 494, 'place': 666, 'safe': 767, 'rule': 763, 'walk': 952, 'cs': 211, 'twice': 916, 'jetway': 476, 'wish': 976, 'juan': 481, 'mess': 567, 'overhead': 636, 'meal': 556, 'fall': 323, 'notification': 611, 'tired': 888, 'useless': 939, 'hey': 425, 'folk': 363, 'level': 505, 'explanation': 314, 'lady': 491, 'baby': 77, 'fyi': 390, 'carrier': 128, 'boston': 104, 'site': 805, 'track': 898, 'lhr': 507, 'expensive': 310, 'rock': 754, 'sea': 776, 'meet': 561, 'pretty': 689, 'remember': 731, 'saturday': 771, 'consider': 191, 'program': 698, 'handle': 410, 'tsa': 911, 'pre': 685, 'entire': 293, 'charlotte': 141, 'previous': 690, 'office': 617, 'tho': 882, 'kill': 485, 'feel': 335, 'slc': 808, 'hit': 429, 'suitcase': 852, 'silver': 801, 'platinum': 669, 'fantastic': 326, 'item': 470, 'multiple': 592, 'diff': 254, 'sound': 821, 'concern': 183, 'assign': 56, 'husband': 445, 'impossible': 453, 'imagine': 451, 'march': 549, 'appreciated': 48, 'pdx': 650, 'equipment': 294, 'special': 826, 'fit': 345, 'video': 944, 'ppl': 683, 'bin': 92, 'visit': 947, 'fill': 339, 'form': 370, 'locate': 522, 'sunday': 854, 'client': 158, 'apply': 46, 'forever': 368, 'rdu': 713, 'click': 157, 'code': 166, 'inside': 463, 'loyal': 535, 'unhelpful': 928, 'set': 790, 'nashville': 595, 'son': 817, 'infant': 458, 'gold': 399, 'game': 391, 'current': 212, 'play': 670, 'single': 802, 'rent': 732, 'runway': 765, 'volume': 948, 'clothe': 160, 'major': 544, 'standby': 832, 'diego': 253, 'prompt': 701, 'okay': 620, 'real': 717, 'plenty': 671, 'couple': 203, 'mind': 578, 'center': 134, 'mark': 550, 'likely': 512, 'kudo': 488, 'state': 834, 'matter': 552, 'nearly': 597, 'window': 974, 'auto': 66, 'callback': 122, 'exist': 307, 'leg': 502, 'photo': 660, 'checked': 144, 'appease': 45, 'resolution': 744, 'fund': 386, 'write': 988, 'compensation': 177, 'delivery': 234, 'reward': 750, 'receipt': 721, 'totally': 896, 'disappointing': 261, 'mechanic': 558, 'winter': 975, 'representative': 737, 'will': 971, 'unhappy': 927, 'willing': 972, 'decide': 229, 'total': 895, 'suggestion': 851, 'deny': 238, 'international': 465, 'asap': 54, 'letter': 504, 'solution': 815, 'listen': 516, 'passbook': 642, 'possibly': 680, 'picture': 663, 'rr': 759, 'light': 510, 'result': 748, 'automatically': 69, 'pr': 684, 'domestic': 271, 'push': 705, 'funeral': 387, 'incredibly': 457, 'freeze': 374, 'tuesday': 912, 'luv': 539, 'waiting': 951, 'angry': 35, 'party': 640, 'girl': 395, 'base': 83, 'coach': 165, 'emergency': 286, 'surprise': 860, 'april': 49, 'aircraft': 22, 'deplane': 242, 'patient': 647, 'frustrate': 379, 'ready': 716, 'plz': 674, 'avoid': 72, 'midnight': 573, 'mon': 585, 'bank': 82, 'svc': 861, 'accept': 4, 'delayed': 232, 'slow': 810, 'complain': 178, 'clearly': 156, 'attempt': 61, 'raise': 710, 'arrival': 52, 'suggest': 850, 'automated': 68, 'estimate': 297, 'battle': 85, 'improve': 454, 'omg': 622, 'announcement': 37, 'inflight': 459, 'shame': 792, 'bs': 110, 'pull': 703, 'tampa': 870, 'ya': 991, 'discount': 263, 'storm': 842, 'one': 624, 'ignore': 450, 'bus': 114, 'outside': 635, 'straight': 844, 'clean': 154, 'elite': 284, 'till': 886, 'medium': 560, 'promo': 700, 'entertainment': 292, 'final': 340, 'uk': 921, 'additional': 13, 'bathroom': 84, 'seattle': 779, 'foot': 366, 'com': 169, 'fair': 322, 'assist': 57, 'match': 551, 'spring': 828, 'ugh': 920, 'usually': 940, 'automate': 67, 'extend': 315, 'social': 814, 'ball': 81, 'double': 274, 'unbelievable': 924, 'comment': 171, 'patience': 646, 'america': 32, 'dia': 250, 'divert': 264, 'austin': 65}\n"
     ]
    }
   ],
   "source": [
    "print(word_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF vectors for Train Set\n",
    "X_train=word_vec.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 7 stored elements and shape (1, 1000)>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF vectors for test Set\n",
    "X_test=word_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7715163934426229"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#training model\n",
    "lr_model=LogisticRegression().fit(X_train,y_train)\n",
    "lr_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"lr_model.pkl\",\"wb\") as f:\n",
    "    pickle.dump(lr_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lr_model.pkl\",\"rb\") as f:\n",
    "    lr_model_downloaded=pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 0 0 0]\n",
      "f1_score 0.760655039079428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred=lr_model_downloaded.predict(X_test)\n",
    "print(y_pred)\n",
    "print(\"f1_score\",f1_score(y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73720137, 0.74914676, 0.7116041 , 0.72136752, 0.74700855])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(LogisticRegression(),X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyser(tweet):\n",
    "    #cleaned tweet\n",
    "    cleaned_tweet=text_claner(tweet)\n",
    "    # print(cleaned_tweet)\n",
    "     # Feature Engineering , tfidf\n",
    "    tweet_vector=word_vec.transform([cleaned_tweet])\n",
    "    # print(tweet_vector)\n",
    "    #predicting sentiment\n",
    "    label=lr_model_downloaded.predict(tweet_vector)\n",
    "    # print(label)\n",
    "    #showing predicted text\n",
    "    return le.inverse_transform(np.array(label))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**Sample Tweet:**</font>\n",
    "<p>@USAirways flt 419. 2+ hrs Late Flight, baggage + 1 more hr. Now I see they delivered my suitcase wet inside &amp; out. #NotHappy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyser(\"@USAirways flt 419. 2+ hrs Late Flight, baggage + 1 more hr. Now I see they delivered my suitcase wet inside &amp; out.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
